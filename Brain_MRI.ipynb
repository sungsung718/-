{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brain MRI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19d_Z3Bx3E4dzgMqTD2UH4mVLipuj5LiD",
      "authorship_tag": "ABX9TyOMlx1aizey08XdGpqDNeuM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungsung718/Applied-Cognitivie-Psychology/blob/main/Brain_MRI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "F_isIkPDbkKt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformation = transforms.Compose([\n",
        "                                     transforms.Grayscale(),\n",
        "                                     transforms.Resize((32, 32)),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Lambda(lambda x: (x / 255.0) - 0.5)\n",
        "                                     ])\n",
        "\n",
        "dataset = ImageFolder(root = \"/content/drive/MyDrive/BrainImage/brain_tumor_dataset\",\n",
        "                                             transform = transformation)\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "test_size = dataset_size - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ],
      "metadata": {
        "id": "LFUteKMiZ-z1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQZtSToWWoC6",
        "outputId": "09bb5632-515b-4018-d979-dd44bd5c53e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.2466158866882324\n",
            "Epoch: 1, Loss: 2.012960910797119\n",
            "Epoch: 2, Loss: 1.7423089742660522\n",
            "Epoch: 3, Loss: 1.4400606155395508\n",
            "Epoch: 4, Loss: 1.3816896677017212\n",
            "Epoch: 5, Loss: 1.2244049310684204\n",
            "Epoch: 6, Loss: 1.0217609405517578\n",
            "Epoch: 7, Loss: 1.3096354007720947\n",
            "Epoch: 8, Loss: 1.0151116847991943\n",
            "Epoch: 9, Loss: 0.8440475463867188\n",
            "Epoch: 10, Loss: 1.0583118200302124\n",
            "Epoch: 11, Loss: 0.7847738265991211\n",
            "Epoch: 12, Loss: 0.8970576524734497\n",
            "Epoch: 13, Loss: 0.8276912569999695\n",
            "Epoch: 14, Loss: 0.7303128242492676\n",
            "Epoch: 15, Loss: 0.7203853130340576\n",
            "Epoch: 16, Loss: 0.8269745111465454\n",
            "Epoch: 17, Loss: 0.8832510709762573\n",
            "Epoch: 18, Loss: 0.6974087953567505\n",
            "Epoch: 19, Loss: 0.676308274269104\n"
          ]
        }
      ],
      "source": [
        "train_loader =torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle =True) #mini batch로 나누기\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 64, shuffle = False, num_workers = 4)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1,16,kernel_size = 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size = 3, padding=1)\n",
        "        self.fc1 = nn.Linear(8*8*8, 32)\n",
        "        self.fc2 = nn.Linear(32,10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.batchnormalize = nn.BatchNorm2d(16)\n",
        "    def forward(self,x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.batchnormalize(out)\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8*8*8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = Net().cuda()\n",
        "LR= 0.01\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.cuda(), labels.cuda()\n",
        "        outputs = model(imgs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad() #gradient을 0으로 초기화\n",
        "        loss.backward() #역전파\n",
        "        optimizer.step() #parameter update\n",
        "    print(f'Epoch: {epoch}, Loss: {float(loss)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.cuda(), labels.cuda()\n",
        "        outputs = model(imgs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G31Sz0EEkwFv",
        "outputId": "1a17efe8-1618-449f-ab5d-aa0d3af572fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60 %\n"
          ]
        }
      ]
    }
  ]
}